{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "input_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7652df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "\n",
    "def load_dataset(filename):\n",
    "    \"\"\"\n",
    "    Load test dataset from file.\n",
    "\n",
    "    Args:\n",
    "        filename: Name of the test data file\n",
    "\n",
    "    Returns:\n",
    "        train_data: Samples to train with shape \n",
    "                  (num_samples, num_timestep, num_channels, num_bands)\n",
    "        test_data: Samples to test with shape \n",
    "                  (num_samples, num_timestep, num_channels, num_bands)\n",
    "        val_data: Samples to validate with shape \n",
    "                  (num_samples, num_timestep, num_channels, num_bands)\n",
    "    \"\"\"\n",
    "    test_file = os.path.join(input_dir, filename)\n",
    "    # Open the file and load the data\n",
    "    # split into train(80%), test(10%), val(10%)\n",
    "    data = np.load(test_file)['arr_0']\n",
    "    train_data = data[:int(len(data)*0.8)]\n",
    "    test_data = data[int(len(data)*0.8):int(len(data)*0.9)]\n",
    "    val_data = data[int(len(data)*0.9):]\n",
    "\n",
    "    return train_data, test_data, val_data\n",
    "\n",
    "\n",
    "def normalize(data, average=[], std=[]):\n",
    "    # normalizing input to the range of [~mean - 4*std, ~mean + 4*std]\n",
    "    # becasue in this dataset, the timestep is in first dimension.\n",
    "    # adapt the normalization to fit this situation, compute mean and std in axis 0\n",
    "    if data.ndim == 4:\n",
    "        n, t, c, f = data.shape\n",
    "        data = data.reshape((n*t, -1))  # neuron input\n",
    "    if len(average) == 0:\n",
    "        average = np.mean(data, axis=0, keepdims=True)\n",
    "        std = np.std(data, axis=0, keepdims=True)\n",
    "    combine_max = average + 4 * std\n",
    "    combine_min = average - 4 * std\n",
    "    norm_data = 2 * (data - combine_min) / (combine_max - combine_min) - 1\n",
    "    norm_data = norm_data.reshape((n, t, c, f))\n",
    "    return norm_data, average, std\n",
    "\n",
    "\n",
    "class NeuroForcastDataset(Dataset):\n",
    "  def __init__(self, neural_data, use_graph=False, average=[], std=[]):\n",
    "    \"\"\"\n",
    "    neural_data: N*T*C*F (sampe size * total time steps * channel *feature dimension)\n",
    "    f_window: T' the length of prediction window\n",
    "    batch_size: batch size\n",
    "    \"\"\"\n",
    "    self.data = neural_data\n",
    "    self.use_graph = use_graph\n",
    "    if len(average) == 0:\n",
    "      self.data, self.average, self.std = normalize(self.data)\n",
    "    else:\n",
    "      self.data, self.average, self.std = normalize(self.data, average, std)\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    data = self.data[index]\n",
    "    if not self.use_graph:\n",
    "      data = data[:, :, 0]\n",
    "\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    return data\n",
    "\n",
    "\n",
    "class NFBaseModel(torch.nn.Module):\n",
    "  def __init__(self, input_size=96, hidden_size=256):\n",
    "    super(NFBaseModel, self).__init__()\n",
    "    self.encoder = torch.nn.GRU(\n",
    "        input_size=input_size, hidden_size=hidden_size, num_layers=2, batch_first=True)\n",
    "    self.output_layer = torch.nn.Linear(hidden_size, input_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    output, hidden = self.encoder(x)\n",
    "    output = self.output_layer(output)\n",
    "    return output\n",
    "\n",
    "  def predict(self, x):\n",
    "    output, hidden = self.encoder(x)\n",
    "    output = self.output_layer(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "  def __init__(self, model, train_data_loader, test_data_loader,\n",
    "               val_data_loader, loss_fn, optimizer, device, scheduler,\n",
    "               forecasting_mode, init_steps=10, save_path='', ckpt_path=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      model: model to train\n",
    "      train_data_loader: train data loader\n",
    "      test_data_loader: test data loader\n",
    "      val_data_loader: validation data loader\n",
    "      loss_fn: loss function\n",
    "      optimizer: optimizer\n",
    "      device: device\n",
    "      forecasting_mode: either 'one_step' or 'multi_step'\n",
    "      init_steps: number of initial steps for multi-step forecasting\n",
    "\n",
    "    \"\"\"\n",
    "    self.model = model\n",
    "    self.train_data_loader = train_data_loader\n",
    "    self.test_data_loader = test_data_loader\n",
    "    self.val_data_loader = val_data_loader\n",
    "\n",
    "    self.loss_fn = loss_fn\n",
    "    self.optimizer = optimizer\n",
    "    self.device = device\n",
    "    self.forecasting_mode = forecasting_mode\n",
    "    self.init_steps = init_steps\n",
    "\n",
    "    self.save_path = save_path\n",
    "    self.ckpt_path = ckpt_path\n",
    "    if ckpt_path is not None:\n",
    "      self.load_model(ckpt_path)\n",
    "    self.model.to(device)\n",
    "    self.scheduler = scheduler\n",
    "\n",
    "    self.train_loss = []\n",
    "    self.test_loss = []\n",
    "    self.val_loss = []\n",
    "\n",
    "  def prepare_data(self, data):\n",
    "    if self.forecasting_mode == 'one_step':\n",
    "      input_data = data[:, :-1, :]\n",
    "      target_data = data[:, 1:, :]\n",
    "    elif self.forecasting_mode == 'multi_step':\n",
    "      future_step = data.shape[1] - self.init_steps\n",
    "      # masking out future dataset\n",
    "\n",
    "      input_data = torch.cat([data[:, :self.init_steps], torch.repeat_interleave(data[:, self.init_steps-1:self.init_steps],\n",
    "                                                                                 future_step, dim=1)], dim=1)\n",
    "      target_data = data[:, self.init_steps:]\n",
    "\n",
    "\n",
    "      # print(f\"input_data shape: {input_data.shape}\")\n",
    "      # print(f\"target_data shape: {target_data.shape}\")\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          'forecasting_mode must be either one_step or multi_step')\n",
    "    return input_data, target_data\n",
    "\n",
    "  def loss_function(self, prediction, target):\n",
    "    if self.forecasting_mode == 'one_step':\n",
    "      loss = self.loss_fn(prediction, target)\n",
    "    else:\n",
    "      loss = self.loss_fn(prediction[:, self.init_steps:], target)\n",
    "    return loss\n",
    "\n",
    "  def train(self, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "      self.model.train()\n",
    "      train_loss = 0.0\n",
    "      for batch in self.train_data_loader:\n",
    "        batch = batch.to(self.device)\n",
    "        input_data, target_data = self.prepare_data(batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(input_data)\n",
    "        loss = self.loss_function(output, target_data)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "      if epoch % 10 == 0:\n",
    "        self.model.eval()\n",
    "        val_loss = self.validation()\n",
    "        self.model.train()\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss / len(self.train_data_loader)}, Val Loss:{val_loss / len(self.val_data_loader)}')\n",
    "\n",
    "  def validation(self):\n",
    "    self.model.eval()\n",
    "    val_loss = 0.0\n",
    "    for batch in self.val_data_loader:\n",
    "      batch = batch.to(self.device)\n",
    "      input_data, target_data = self.prepare_data(batch)\n",
    "      output = self.model(input_data)\n",
    "      loss = self.loss_fn(output[:, self.init_steps:], target_data)\n",
    "      val_loss += loss.item()\n",
    "    return val_loss\n",
    "\n",
    "  def save_model(self):\n",
    "    torch.save(self.model.state_dict(), self.save_path)\n",
    "\n",
    "  def load_model(self, path):\n",
    "    self.model.load_state_dict(torch.load(self.ckpt_path))\n",
    "\n",
    "  def prediction(self):\n",
    "    self.model.eval()\n",
    "    outputs_pred = []\n",
    "    outputs_gt = []\n",
    "    for batch in self.val_data_loader:\n",
    "      batch = batch.to(self.device)\n",
    "      input_data, target_data = self.prepare_data(batch)\n",
    "      output = self.model(input_data)\n",
    "      outputs_pred.append(output.detach().cpu().numpy())\n",
    "      outputs_gt.append(target_data.detach().cpu().numpy())\n",
    "    return outputs_pred, outputs_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb9859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "# dataset_name = 'beignet'\n",
    "dataset_name = 'affi'\n",
    "if dataset_name == 'beignet':\n",
    "  num_channels = 89\n",
    "else:\n",
    "  num_channels = 239\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "hidden_size = 1024\n",
    "input_size = num_channels\n",
    "\n",
    "# define dataloader\n",
    "train_data, test_data, val_data = load_dataset(\n",
    "    f'train_data_{dataset_name}.npz')  # B * T * C * F\n",
    "\n",
    "# print the shape of the train, test, val data\n",
    "print(f'train_data shape: {train_data.shape}')\n",
    "print(f'test_data shape: {test_data.shape}')\n",
    "print(f'val_data shape: {val_data.shape}')\n",
    "\n",
    "# plot test_data curve for a few channels\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(test_data[0, :, 0, 1])\n",
    "plt.plot(test_data[0, :, 1, 1])\n",
    "plt.plot(test_data[0, :, 2, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75651d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define dataloader\n",
    "\n",
    "train_data = NeuroForcastDataset(train_data, use_graph=False)\n",
    "\n",
    "# Save train_data average and std one file\n",
    "np.savez(f'train_data_average_std_{dataset_name}.npz', average=train_data.average, std=train_data.std)\n",
    "\n",
    "test_data = NeuroForcastDataset(test_data, use_graph=False, average=train_data.average, std=train_data.std)\n",
    "val_data = NeuroForcastDataset(val_data, use_graph=False, average=train_data.average, std=train_data.std)\n",
    "train_data_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_data_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "val_data_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "model = NFBaseModel(hidden_size=hidden_size, input_size=input_size)\n",
    "model = model.to(torch.device('mps'))\n",
    "loss_fn = torch.nn.MSELoss('mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lambda1 = lambda ith_epoch: 0.95 ** (ith_epoch // 50)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "\n",
    "\n",
    "trainer = Trainer(model, train_data_loader, test_data_loader,\n",
    "               val_data_loader, loss_fn, optimizer, device= torch.device('mps'), scheduler=scheduler,\n",
    "               forecasting_mode='multi_step', init_steps = 10, save_path ='./model.pth', ckpt_path=None)\n",
    "\n",
    "trainer.train(100)\n",
    "\n",
    "#save the model\n",
    "torch.save(model.state_dict(), f'model_{dataset_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one test event and compare the output with the target\n",
    "\n",
    "# load the model\n",
    "model_test = NFBaseModel(hidden_size=hidden_size, input_size=input_size)\n",
    "model_test = model_test.to(torch.device('mps'))\n",
    "model_test.load_state_dict(torch.load(f'./model_{dataset_name}.pth'))\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    init_steps = 10\n",
    "    # forecasting_mode == 'multi_step':\n",
    "    future_step = data.shape[1] - init_steps\n",
    "    # masking out future dataset\n",
    "\n",
    "    input_data = torch.cat([data[:, :init_steps], torch.repeat_interleave(\n",
    "        data[:, init_steps-1:init_steps], future_step, dim=1)], dim=1)\n",
    "    target_data = data\n",
    "\n",
    "    return input_data, target_data\n",
    "\n",
    "\n",
    "# define r2_score and mse_score\n",
    "def r2_score(target_data, output):\n",
    "    return 1 - (torch.sum((target_data - output) ** 2) / torch.sum(target_data ** 2))\n",
    "\n",
    "\n",
    "def mse_score(target_data, output):\n",
    "    return torch.mean((target_data - output) ** 2)\n",
    "\n",
    "\n",
    "# define r2_score and mse_score using numpy\n",
    "def r2_score_np(target_data, output):\n",
    "    return 1 - (np.sum((target_data - output) ** 2) / np.sum(target_data ** 2))\n",
    "\n",
    "\n",
    "def mse_score_np(target_data, output):\n",
    "    return np.mean((target_data - output) ** 2)\n",
    "\n",
    "# Run model inference and collect predictions\n",
    "\n",
    "\n",
    "def run_model_inference(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Run model inference on test data and collect predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: Test data loader\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Model predictions as numpy array\n",
    "        targets: Ground truth targets as numpy array\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Prepare input and target data\n",
    "            input_data, target_data = prepare_data(batch)\n",
    "            output = model(input_data)\n",
    "\n",
    "            # Store predictions and targets\n",
    "            predictions.append(output.cpu().numpy())\n",
    "            targets.append(target_data.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "\n",
    "    return predictions, targets\n",
    "\n",
    "\n",
    "# Run inference\n",
    "predictions, targets = run_model_inference(\n",
    "    model_test, test_data_loader, torch.device('mps'))\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "r2 = r2_score(torch.tensor(targets), torch.tensor(predictions))\n",
    "mse = mse_score(torch.tensor(targets), torch.tensor(predictions))\n",
    "\n",
    "# Calculate evaluation metrics using numpy\n",
    "r2_np = r2_score_np(np.array(targets), np.array(predictions))\n",
    "mse_np = mse_score_np(np.array(targets), np.array(predictions))\n",
    "\n",
    "print(f'Test R2 score: {r2:.4f}')\n",
    "print(f'Test MSE score: {mse:.4f}')\n",
    "print(f'Test R2 score(NP): {r2_np:.4f}')\n",
    "print(f'Test MSE score(NP): {mse_np:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the output and target data for 1st channel\n",
    "\n",
    "\n",
    "channel_idx = 1\n",
    "\n",
    "def plot_channel(channel_idx):\n",
    "  # plot the output and target data (dotted line)\n",
    "  # output_data_list start from x = 10 time steps\n",
    "  plt.plot(target_data_list[0, :, channel_idx], linestyle='--', label='target')\n",
    "  # Plot output data starting from the 10th time step\n",
    "  x_output = np.arange(10, len(output_data_list[0, :, channel_idx]))\n",
    "  plt.plot(x_output, output_data_list[0, 10:, channel_idx], linestyle='-', label='output')\n",
    "\n",
    "  # Draw a vertical line at the 10th time step\n",
    "  plt.axvline(x=10, color='black', linestyle='--')\n",
    "  plt.legend(['output', 'target'])\n",
    "  plt.title(f'Channel {channel_idx}')\n",
    "  plt.xlabel('Time')\n",
    "  plt.ylabel('lfp')\n",
    "  plt.show()\n",
    "\n",
    "plot_channel(1)\n",
    "plot_channel(2)\n",
    "plot_channel(3)\n",
    "plot_channel(4)\n",
    "plot_channel(5)\n",
    "plot_channel(6)\n",
    "plot_channel(7)\n",
    "plot_channel(8)\n",
    "plot_channel(9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
